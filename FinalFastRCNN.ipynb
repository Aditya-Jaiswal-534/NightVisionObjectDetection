{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b43aa40219a5498fb452198da9ac11f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_382b9256d1984790a0a53bd12ae740e8","IPY_MODEL_a1ae53e0e8104a13b0cceb82c7dec793","IPY_MODEL_6e10b0ec6db3478290d10e53ed7e8dfb"],"layout":"IPY_MODEL_c792e28907e445b1a7ae3f0baaf67c82"}},"382b9256d1984790a0a53bd12ae740e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de4e4a42998d4ea5ad2ee2d87301e978","placeholder":"​","style":"IPY_MODEL_92fe2ebc356a485995d5d4ffc901ea07","value":"100%"}},"a1ae53e0e8104a13b0cceb82c7dec793":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_969992cdc4744657a211c4517b34e025","max":6471,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aea665ae8a3c4bdc91895008ff3bbb7a","value":6471}},"6e10b0ec6db3478290d10e53ed7e8dfb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8530ee6d75b04caa9711d23505a2611e","placeholder":"​","style":"IPY_MODEL_d1b8a630d1fe4cf78447d00d7002e5e4","value":" 6471/6471 [00:09&lt;00:00, 978.42it/s]"}},"c792e28907e445b1a7ae3f0baaf67c82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de4e4a42998d4ea5ad2ee2d87301e978":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92fe2ebc356a485995d5d4ffc901ea07":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"969992cdc4744657a211c4517b34e025":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aea665ae8a3c4bdc91895008ff3bbb7a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8530ee6d75b04caa9711d23505a2611e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1b8a630d1fe4cf78447d00d7002e5e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"810573928e044a278554f4f3362a8175":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3fafa843a8b042e68891229fcaa18201","IPY_MODEL_843ecb26e8784237bf7f59221be74a9f","IPY_MODEL_a52734d7f24c409fb97a766a976d919e"],"layout":"IPY_MODEL_81c5b085810c4586b9a5bdf9232df0bc"}},"3fafa843a8b042e68891229fcaa18201":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_498619f4f43b4f3181f01c2e62a7efe6","placeholder":"​","style":"IPY_MODEL_e3fa6155109c4beaa7deecd2a2acb69b","value":"100%"}},"843ecb26e8784237bf7f59221be74a9f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52cdf20aa9774d2ab0e6da4d2fb9fa22","max":548,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c6b2d072c51e47dfbb6653b40081a1aa","value":548}},"a52734d7f24c409fb97a766a976d919e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2829c86efe804e71aab0f64a45fd7262","placeholder":"​","style":"IPY_MODEL_8b455a4187414ed2b79ae828bdaa096d","value":" 548/548 [00:00&lt;00:00, 965.89it/s]"}},"81c5b085810c4586b9a5bdf9232df0bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"498619f4f43b4f3181f01c2e62a7efe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3fa6155109c4beaa7deecd2a2acb69b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52cdf20aa9774d2ab0e6da4d2fb9fa22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b2d072c51e47dfbb6653b40081a1aa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2829c86efe804e71aab0f64a45fd7262":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b455a4187414ed2b79ae828bdaa096d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0eea93e3d2c429cb474a484e35d55de":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_46cb6a810bb94c2c868456b1685e37c6","IPY_MODEL_476405fc0e004e5d9f7555b3f008f8c9","IPY_MODEL_00fb3e5a3d614b5ba0dfc0a1630382d5"],"layout":"IPY_MODEL_76156cf37e06434180ec7501f9ecf239"}},"46cb6a810bb94c2c868456b1685e37c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85bee118fa494200a662c2156d83052a","placeholder":"​","style":"IPY_MODEL_d5eaa9a9e2644762afac5fef4f85adaa","value":"100%"}},"476405fc0e004e5d9f7555b3f008f8c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_42b96df305164b85b5094220d18e3ac5","max":6471,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8df0cb76d2b044c096682b127dac2c91","value":6471}},"00fb3e5a3d614b5ba0dfc0a1630382d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7c2c0cdb87b4a55b6b18380e6dc2fe8","placeholder":"​","style":"IPY_MODEL_1261b1d2dbf14d32a2169d215163838a","value":" 6471/6471 [00:05&lt;00:00, 1604.55it/s]"}},"76156cf37e06434180ec7501f9ecf239":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85bee118fa494200a662c2156d83052a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5eaa9a9e2644762afac5fef4f85adaa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42b96df305164b85b5094220d18e3ac5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8df0cb76d2b044c096682b127dac2c91":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7c2c0cdb87b4a55b6b18380e6dc2fe8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1261b1d2dbf14d32a2169d215163838a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a026dc0d94dc420582c3b5d6d6a346e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c2618cc27a0469cb1859af938b08463","IPY_MODEL_988eca5ef04347bebdc9f6ab8bf4d9ba","IPY_MODEL_1c5a0026ef09494abcdcb9c3cab6e742"],"layout":"IPY_MODEL_2191a736c7d44139b4dc78c84157efc3"}},"3c2618cc27a0469cb1859af938b08463":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64afc158037f443ba83dca805b9f0d7f","placeholder":"​","style":"IPY_MODEL_195f7a0358224cb5962c8eea63b242f1","value":"100%"}},"988eca5ef04347bebdc9f6ab8bf4d9ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f74318166cf4574b62a5a106e9d2c07","max":548,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6928fd2e15647d2a605aa0cb5de1e32","value":548}},"1c5a0026ef09494abcdcb9c3cab6e742":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4156a9b270940f38499dff5ba160500","placeholder":"​","style":"IPY_MODEL_038926f45a4e4bed913a05530ad14042","value":" 548/548 [00:00&lt;00:00, 2417.98it/s]"}},"2191a736c7d44139b4dc78c84157efc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64afc158037f443ba83dca805b9f0d7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"195f7a0358224cb5962c8eea63b242f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f74318166cf4574b62a5a106e9d2c07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6928fd2e15647d2a605aa0cb5de1e32":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b4156a9b270940f38499dff5ba160500":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"038926f45a4e4bed913a05530ad14042":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13762455,"sourceType":"datasetVersion","datasetId":8758138}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- CELL 1: MASTER SETUP (KAGGLE) ---\n\n# 1. Install YOLOv8\n# (Make sure \"Internet\" is enabled in your Kaggle notebook settings)\n!pip install -q ultralytics\n\n# 2. Check for the T4 GPU\nprint(\"--- Checking GPU ---\")\n!nvidia-smi -L\nprint(\"--------------------\")\n\n# 3. Import ALL necessary libraries\nimport os\nimport zipfile\nimport shutil\nimport glob\nfrom tqdm.auto import tqdm\nfrom ultralytics import YOLO\nfrom PIL import Image  # This is for opening/reading images\n# (IPython.display and Google Drive imports removed)\n\nprint(\"✅ Libraries installed and imported correctly.\")\n\n# 4. (Google Drive mount removed)\n\n# 5. Define base paths\n# Replace '<your-dataset-name>' with the name of your dataset\ninput_data_path = '/kaggle/input/visdrone'\noutput_working_path = '/kaggle/working/'\n\nprint(f\"Data will be read from: {input_data_path}\")\nprint(f\"Output files will be saved to: {output_working_path}\")\nprint(\"✅ Paths defined.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2q6LDBPtNjC","outputId":"9cb6f072-0403-417f-bb8a-208253a3cc01","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:46:30.353625Z","iopub.execute_input":"2025-11-17T10:46:30.354461Z","iopub.status.idle":"2025-11-17T10:47:52.720430Z","shell.execute_reply.started":"2025-11-17T10:46:30.354427Z","shell.execute_reply":"2025-11-17T10:47:52.719673Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m--- Checking GPU ---\nGPU 0: Tesla T4 (UUID: GPU-5d553db9-8a16-de34-e179-2cdd735b6e93)\nGPU 1: Tesla T4 (UUID: GPU-121caeda-0150-284d-f721-c469d6eb27ce)\n--------------------\nCreating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n✅ Libraries installed and imported correctly.\nData will be read from: /kaggle/input/visdrone\nOutput files will be saved to: /kaggle/working/\n✅ Paths defined.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import zipfile\nimport os\n\n# --- Define Zip File Paths ---\n# Assumes 'input_data_path' was defined in the previous cell\ntrain_zip_path = os.path.join(input_data_path, 'modified_visdrone_scaled_annos.zip')\nval_zip_path = os.path.join(input_data_path, 'modified_visdrone_scaled_annos_val.zip')\n\n# --- Define Unzip Destinations ---\nbase_unzip_dir = '/kaggle/working/visdrone_data'\ntrain_dest = os.path.join(base_unzip_dir, 'train_data')\nval_dest = os.path.join(base_unzip_dir, 'val_data')\n\n# Create the base directory if it doesn't exist\nos.makedirs(base_unzip_dir, exist_ok=True)\n\n# --- Unzip Training Data ---\nprint(f\"Unzipping {train_zip_path}...\")\nif os.path.exists(train_zip_path):\n    with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n        zip_ref.extractall(base_unzip_dir)\n        \n        extracted_folder = os.path.join(base_unzip_dir, 'modified_visdrone_scaled_annos')\n        if os.path.exists(extracted_folder):\n            os.rename(extracted_folder, train_dest)\n            print(\"Training data unzipped.\")\n        else:\n            print(f\"Warning: Expected folder '{extracted_folder}' not found after extraction.\")\nelse:\n    print(f\"❌ ERROR: {train_zip_path} not found!\")\n\n# --- Unzip Validation Data ---\nprint(f\"Unzipping {val_zip_path}...\")\nif os.path.exists(val_zip_path):\n    with zipfile.ZipFile(val_zip_path, 'r') as zip_ref:\n        zip_ref.extractall(base_unzip_dir)\n        \n        extracted_folder_val = os.path.join(base_unzip_dir, 'modified_visdrone_scaled_annos_val')\n        if os.path.exists(extracted_folder_val):\n            os.rename(extracted_folder_val, val_dest)\n            print(\"Validation data unzipped.\")\n        else:\n             print(f\"Warning: Expected folder '{extracted_folder_val}' not found after extraction.\")\nelse:\n    print(f\"❌ ERROR: {val_zip_path} not found!\")\n\nprint(\"\\n✅ Datasets unzipped to /kaggle/working/visdrone_data\")\n\n/kaggle/input/visdrone/modified_visdrone_scaled_annos","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k_NNmecjtDkf","outputId":"1bf48dfa-115e-410f-d0e1-89e70e5d2617","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:48:34.303755Z","iopub.execute_input":"2025-11-17T10:48:34.304184Z","iopub.status.idle":"2025-11-17T10:48:34.330805Z","shell.execute_reply.started":"2025-11-17T10:48:34.304155Z","shell.execute_reply":"2025-11-17T10:48:34.329861Z"}},"outputs":[{"name":"stdout","text":"Unzipping /kaggle/input/visdrone/modified_visdrone_scaled_annos.zip...\n❌ ERROR: /kaggle/input/visdrone/modified_visdrone_scaled_annos.zip not found!\nUnzipping /kaggle/input/visdrone/modified_visdrone_scaled_annos_val.zip...\n❌ ERROR: /kaggle/input/visdrone/modified_visdrone_scaled_annos_val.zip not found!\n\n✅ Datasets unzipped to /kaggle/working/visdrone_data\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/1235582437.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n✅ Datasets unzipped to /kaggle/working/visdrone_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mkaggle\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvisdrone\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmodified_visdrone_scaled_annos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'kaggle' is not defined"],"ename":"NameError","evalue":"name 'kaggle' is not defined","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"# --- CELL 3: CONVERT ANNOTATIONS (KAGGLE) ---\nimport json\nimport glob\nimport shutil\nfrom tqdm.auto import tqdm\nfrom PIL import Image\n\n# --- 1. Define VisDrone categories ---\n# These are the 10 classes we care about, matching the original script's logic\n# (skipping 0: 'ignored' and 11: 'other')\nVISDRONE_CATEGORIES = [\n    {\"id\": 1, \"name\": \"pedestrian\", \"supercategory\": \"person\"},\n    {\"id\": 2, \"name\": \"person\", \"supercategory\": \"person\"},\n    {\"id\": 3, \"name\": \"bicycle\", \"supercategory\": \"vehicle\"},\n    {\"id\": 4, \"name\": \"car\", \"supercategory\": \"vehicle\"},\n    {\"id\": 5, \"name\": \"van\", \"supercategory\": \"vehicle\"},\n    {\"id\": 6, \"name\": \"truck\", \"supercategory\": \"vehicle\"},\n    {\"id\": 7, \"name\": \"tricycle\", \"supercategory\": \"vehicle\"},\n    {\"id\": 8, \"name\": \"awning-tricycle\", \"supercategory\": \"vehicle\"},\n    {\"id\": 9, \"name\": \"bus\", \"supercategory\": \"vehicle\"},\n    {\"id\": 10, \"name\": \"motor\", \"supercategory\": \"vehicle\"},\n]\n\n# --- 2. Define Source and Destination Paths ---\n# Data was unzipped to /kaggle/working/ in the previous cell\nvisdrone_train_dir = '/kaggle/input/visdrone/modified_visdrone_scaled_annos/modified_visdrone_scaled_annos'\nvisdrone_val_dir = '/kaggle/input/visdrone/modified_visdrone_scaled_annos_val/modified_visdrone_scaled_annos_val'\n# All new files must be written to /kaggle/working/\ncoco_base_dir = '/kaggle/working/coco_dataset'\n\n# --- 3. Create the COCO Directory Structure ---\n# This structure is common for COCO datasets\nos.makedirs(os.path.join(coco_base_dir, 'images/train'), exist_ok=True)\nos.makedirs(os.path.join(coco_base_dir, 'images/val'), exist_ok=True)\nos.makedirs(os.path.join(coco_base_dir, 'annotations'), exist_ok=True)\n\nprint(\"COCO directory structure created in /kaggle/working/.\")\n\n\n\n# --- 4. Define the Conversion Function ---\ndef convert_visdrone_to_coco(source_dir, dest_base_dir, split, categories):\n    \"\"\"\n    Converts VisDrone annotations to COCO format (for Faster R-CNN).\n\n    Args:\n        source_dir (str): Path to the source VisDrone data (e.g., '/kaggle/working/visdrone_data/train_data')\n        dest_base_dir (str): Path to the root of the destination COCO dataset (e.g., '/kaggle/working/coco_dataset')\n        split (str): 'train' or 'val'\n        categories (list): The list of category dictionaries.\n    \"\"\"\n    source_images_dir = os.path.join(source_dir, 'images')\n    source_annos_dir = os.path.join(source_dir, 'annotations')\n\n    dest_images_dir = os.path.join(dest_base_dir, 'images', split)\n\n    # Initialize the main COCO data structure\n    coco_data = {\n        \"images\": [],\n        \"annotations\": [],\n        \"categories\": categories\n    }\n\n    # Keep track of annotation IDs\n    annotation_id_counter = 1\n\n    anno_files = glob.glob(os.path.join(source_annos_dir, '*.txt'))\n    print(f\"\\nConverting {split} data ({len(anno_files)} files) to COCO format...\")\n\n    # Use enumerate to create a unique, 1-based image_id for COCO\n    for image_id, anno_file in enumerate(tqdm(anno_files), 1):\n        base_filename = os.path.basename(anno_file)\n        img_filename = os.path.splitext(base_filename)[0] + '.jpg'\n        img_path = os.path.join(source_images_dir, img_filename)\n\n        # 1. Copy the image\n        if os.path.exists(img_path):\n            shutil.copy(img_path, os.path.join(dest_images_dir, img_filename))\n        else:\n            print(f\"Warning: Image not found for {base_filename}, skipping.\")\n            continue\n\n        # 2. Read image dimensions\n        try:\n            with Image.open(img_path) as img:\n                img_w, img_h = img.size\n        except Exception as e:\n            print(f\"Error opening image {img_path}: {e}. Skipping.\")\n            continue\n\n        # 3. Add image info to COCO data\n        coco_data[\"images\"].append({\n            \"id\": image_id,\n            \"file_name\": img_filename, # Just the filename\n            \"width\": img_w,\n            \"height\": img_h\n        })\n\n        # 4. Read VisDrone annotations and convert to COCO annotations\n        with open(anno_file, 'r') as f:\n            for line in f:\n                parts = line.strip().split(',')\n\n                # VisDrone class (1-based)\n                vis_class = int(parts[5])\n\n                # Skip classes 0 ('ignored') and 11 ('other') as per original logic\n                if vis_class in [0, 11]:\n                    continue\n\n                # This is the category_id for COCO, which we've defined from 1-10\n                category_id = vis_class\n\n                # Read VisDrone bounding box (unnormalized)\n                x_min = int(parts[0])\n                y_min = int(parts[1])\n                w = int(parts[2])\n                h = int(parts[3])\n\n                # COCO format requires [x_min, y_min, width, height]\n                bbox = [x_min, y_min, w, h]\n\n                # COCO format also requires area\n                area = w * h\n\n                # Add the annotation\n                coco_data[\"annotations\"].append({\n                    \"id\": annotation_id_counter,\n                    \"image_id\": image_id,\n                    \"category_id\": category_id,\n                    \"bbox\": bbox,\n                    \"area\": area,\n                    \"iscrowd\": 0, # Assuming not a crowd object\n                    \"segmentation\": [] # Required, but can be empty for bbox\n                })\n\n                annotation_id_counter += 1\n\n    # 5. Write the final COCO JSON annotation file\n    json_path = os.path.join(dest_base_dir, 'annotations', f'instances_{split}.json')\n\n    with open(json_path, 'w') as f_out:\n        json.dump(coco_data, f_out, indent=4)\n\n# --- 5. Run the conversion for both splits ---\nconvert_visdrone_to_coco(visdrone_train_dir, coco_base_dir, 'train', VISDRONE_CATEGORIES)\nconvert_visdrone_to_coco(visdrone_val_dir, coco_base_dir, 'val', VISDRONE_CATEGORIES)\n\nprint(\"\\n✅ Annotation conversion to COCO format complete.\")\nprint(f\"Your dataset is ready in: {coco_base_dir}\")\nprint(f\"Training annotations: {coco_base_dir}/annotations/instances_train.json\")\nprint(f\"Validation annotations: {coco_base_dir}/annotations/instances_val.json\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255,"referenced_widgets":["a0eea93e3d2c429cb474a484e35d55de","46cb6a810bb94c2c868456b1685e37c6","476405fc0e004e5d9f7555b3f008f8c9","00fb3e5a3d614b5ba0dfc0a1630382d5","76156cf37e06434180ec7501f9ecf239","85bee118fa494200a662c2156d83052a","d5eaa9a9e2644762afac5fef4f85adaa","42b96df305164b85b5094220d18e3ac5","8df0cb76d2b044c096682b127dac2c91","b7c2c0cdb87b4a55b6b18380e6dc2fe8","1261b1d2dbf14d32a2169d215163838a","a026dc0d94dc420582c3b5d6d6a346e9","3c2618cc27a0469cb1859af938b08463","988eca5ef04347bebdc9f6ab8bf4d9ba","1c5a0026ef09494abcdcb9c3cab6e742","2191a736c7d44139b4dc78c84157efc3","64afc158037f443ba83dca805b9f0d7f","195f7a0358224cb5962c8eea63b242f1","3f74318166cf4574b62a5a106e9d2c07","e6928fd2e15647d2a605aa0cb5de1e32","b4156a9b270940f38499dff5ba160500","038926f45a4e4bed913a05530ad14042"]},"id":"6xPdoZWWtWHZ","outputId":"94c1a36c-a357-47a9-d9a8-a7f795892782","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:48:39.478932Z","iopub.execute_input":"2025-11-17T10:48:39.479253Z","iopub.status.idle":"2025-11-17T10:49:53.248741Z","shell.execute_reply.started":"2025-11-17T10:48:39.479229Z","shell.execute_reply":"2025-11-17T10:49:53.247960Z"}},"outputs":[{"name":"stdout","text":"COCO directory structure created in /kaggle/working/.\n\nConverting train data (6471 files) to COCO format...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6471 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76d7b9c5f947472b80d0d5b1bcb6b3e5"}},"metadata":{}},{"name":"stdout","text":"\nConverting val data (548 files) to COCO format...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/548 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11597dad112c4125802298fcd7d319d9"}},"metadata":{}},{"name":"stdout","text":"\n✅ Annotation conversion to COCO format complete.\nYour dataset is ready in: /kaggle/working/coco_dataset\nTraining annotations: /kaggle/working/coco_dataset/annotations/instances_train.json\nValidation annotations: /kaggle/working/coco_dataset/annotations/instances_val.json\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\n# --- CHANGE 1: Import MobileNet model and weights ---\nfrom torchvision.models.detection import (\n    fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights,\n    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n)\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.transforms import v2 as T\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# --- 1. Custom Dataset for COCO ---\n# We need this to correctly format the targets (annotations) for the model\nclass CustomCocoDataset(torchvision.datasets.CocoDetection):\n    def __init__(self, root, annFile, transform=None):\n        super(CustomCocoDataset, self).__init__(root, annFile)\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        # Load the raw PIL image and annotation list\n        img, target_list = super(CustomCocoDataset, self).__getitem__(idx)\n\n        # 1. Convert PIL Image to Tensor\n        if self.transform is not None:\n            img = self.transform(img)\n\n        # 2. Convert annotation list to the required tensor dictionary\n        target = {}\n        image_id = torch.tensor([self.ids[idx]]) # Get the original COCO image_id\n\n        # Handle images with no annotations\n        if not target_list:\n            target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n            target[\"labels\"] = torch.zeros(0, dtype=torch.int64)\n            target[\"image_id\"] = image_id\n            target[\"area\"] = torch.zeros(0, dtype=torch.float32)\n            target[\"iscrowd\"] = torch.zeros(0, dtype=torch.int64)\n            return img, target\n\n        # Parse all annotations for this image\n        boxes = [obj['bbox'] for obj in target_list]\n        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n        # Convert [x, y, w, h] to [x1, y1, x2, y2]\n        boxes[:, 2:] += boxes[:, :2]\n\n        labels = [obj['category_id'] for obj in target_list]\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        area = torch.as_tensor([obj['area'] for obj in target_list], dtype=torch.float32)\n        iscrowd = torch.as_tensor([obj['iscrowd'] for obj in target_list], dtype=torch.int64)\n\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        return img, target\n\n# --- 2. Collate Function ---\n# This is necessary to batch images and targets correctly\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n# --- 3. Model Definition ---\ndef get_faster_rcnn_model(num_classes):\n    \"\"\"\n    Loads a pre-trained Faster R-CNN model and replaces the classifier head.\n    \"\"\"\n    # --- CHANGE 2: Use the lighter MobileNetV3 backbone ---\n    # Load a model pre-trained on COCO\n    # model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT) # <-- Original ResNet-50\n    model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT) # <-- New MobileNet\n    print(\"Using lightweight MobileNetV3-Large backbone.\")\n\n    # Get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n\n    # Replace the pre-trained head with a new one\n    # num_classes includes your 10 classes + 1 background class\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model\n\n# --- 4. Main Training Script ---\ndef main():\n    # --- Parameters ---\n    # --- KAGGLE CHANGE ---\n    # These paths come from your 'convert_to_coco.py' script, now in /kaggle/working/\n    TRAIN_IMG_DIR = '/kaggle/working/coco_dataset/images/train'\n    TRAIN_ANN_FILE = '/kaggle/working/coco_dataset/annotations/instances_train.json'\n\n    # We will use 10 classes + 1 background class = 11\n    NUM_CLASSES = 11\n\n    # Parameters from your YOLO script\n    EPOCHS = 20\n\n    # --- CHANGE 3: Restore BATCH_SIZE (model is lighter) ---\n    # The new lightweight model and smaller image size should allow for a larger batch.\n    BATCH_SIZE = 16 # Back to original 16\n    ACCUMULATION_STEPS = 1 # 1 means no accumulation\n    print(f\"Using BATCH_SIZE={BATCH_SIZE} with ACCUMULATION_STEPS={ACCUMULATION_STEPS} (effective batch size: {BATCH_SIZE * ACCUMULATION_STEPS})\")\n\n    # Project paths from your YOLO script\n    # --- KAGGLE CHANGE ---\n    # All outputs must be saved to /kaggle/working/\n    PROJECT_DIR = '/kaggle/working/runs/detect'\n    NAME = 'finetune_visdrone_night_scaled_FASTER_RCNN' # New name for new model\n    OUTPUT_DIR = os.path.join(PROJECT_DIR, NAME)\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    # Set device (GPU or CPU)\n    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n    print(f\"Using device: {device}\")\n\n    # --- Dataset and DataLoader ---\n\n    # --- CHANGE 4: Add Image Resizing (matches YOLO's imgsz=256) ---\n    # This will dramatically speed up training and reduce memory.\n    print(\"Applying transform: Resizing all images to 256x256.\")\n    dataset_train = CustomCocoDataset(\n        root=TRAIN_IMG_DIR,\n        annFile=TRAIN_ANN_FILE,\n        transform=T.Compose([\n            T.ToImage(),\n            T.Resize((256, 256), antialias=True), # <-- Add resize\n            T.ToDtype(torch.float32, scale=True)\n        ])\n    )\n\n    data_loader_train = DataLoader(\n        dataset_train,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=2,\n        collate_fn=collate_fn\n    )\n\n    # --- Model, Optimizer, Scheduler ---\n    print(\"Loading Faster R-CNN model...\")\n    model = get_faster_rcnn_model(NUM_CLASSES)\n    model.to(device)\n\n    # Optimizer\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n    # Learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n    # --- Check for pre-trained weights (Optional) ---\n    # This script fine-tunes from the default ResNet50 backbone.\n    # If you had a 'best.pt' for Faster R-CNN, you would load it here.\n    # We are starting from the public pre-trained model instead.\n    print(f\"✅ Model loaded and ready for fine-tuning.\")\n    print(\"Starting fine-tuning on T4 GPU...\")\n\n    # --- Training Loop ---\n    for epoch in range(EPOCHS):\n        model.train()\n        epoch_loss = 0\n\n        # Zero the gradients once at the start of the epoch\n        optimizer.zero_grad()\n\n        # Use tqdm for a nice progress bar\n        # loop = tqdm(data_loader_train, desc=f\"Epoch {epoch+1}/{EPOCHS} [Training]\")\n        loop = tqdm(enumerate(data_loader_train), desc=f\"Epoch {epoch+1}/{EPOCHS} [Training]\", total=len(data_loader_train))\n\n        for i, (images, targets) in loop:\n            # Move data to the correct device\n            images = list(img.to(device) for img in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            # Forward pass (returns a dict of losses in train mode)\n            loss_dict = model(images, targets)\n\n            # Sum all losses\n            losses = sum(loss for loss in loss_dict.values())\n\n            # --- Gradient Accumulation ---\n            # (No longer needed, ACCUMULATION_STEPS = 1)\n            losses = losses / ACCUMULATION_STEPS\n            losses.backward()\n            if (i + 1) % ACCUMULATION_STEPS == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n            # --- End Gradient Accumulation ---\n            epoch_loss += losses.item() * ACCUMULATION_STEPS\n            loop.set_postfix(loss=losses.item()) # Show the normalized loss\n\n        # --- End of Epoch ---\n        if (len(data_loader_train) % ACCUMULATION_STEPS) != 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n        # Step the learning rate scheduler\n        lr_scheduler.step()\n\n        # avg_loss = epoch_loss / len(data_loader_train)\n        avg_loss = epoch_loss / len(data_loader_train.dataset) # More accurate loss\n        print(f\"\\nEpoch {epoch+1}/{EPOCHS} - Average Training Loss: {avg_loss:.4f}\")\n\n        # Save a checkpoint\n        if (epoch + 1) % 10 == 0 or (epoch + 1) == EPOCHS:\n            checkpoint_path = os.path.join(OUTPUT_DIR, f'model_epoch_{epoch+1}.pt')\n            torch.save(model.state_dict(), checkpoint_path)\n            print(f\"Saved checkpoint: {checkpoint_path}\")\n\n    print(\"✅ Training complete.\")\n\n    final_model_path = os.path.join(OUTPUT_DIR, 'faster_rcnn_final.pt')\n    torch.save(model.state_dict(), final_model_path) # <-- CORRECTED\n    print(f\"Final model saved to: {final_model_path}\")\n\n# Run the training\nif __name__ == \"__main__\":\n    main()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FkWjPt7TuRD_","outputId":"62a7a628-5b8e-44d3-b189-1399f6c6b5d5","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T10:50:05.252618Z","iopub.execute_input":"2025-11-17T10:50:05.253224Z","iopub.status.idle":"2025-11-17T12:33:19.657843Z","shell.execute_reply.started":"2025-11-17T10:50:05.253199Z","shell.execute_reply":"2025-11-17T12:33:19.656909Z"}},"outputs":[{"name":"stdout","text":"Using BATCH_SIZE=16 with ACCUMULATION_STEPS=1 (effective batch size: 16)\nUsing device: cuda:0\nApplying transform: Resizing all images to 256x256.\nloading annotations into memory...\nDone (t=2.03s)\ncreating index...\nindex created!\nLoading Faster R-CNN model...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\n100%|██████████| 74.2M/74.2M [00:00<00:00, 153MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Using lightweight MobileNetV3-Large backbone.\n✅ Model loaded and ready for fine-tuning.\nStarting fine-tuning on T4 GPU...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20 [Training]: 100%|██████████| 405/405 [05:04<00:00,  1.33it/s, loss=1.24]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/20 - Average Training Loss: 0.0876\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 2/20 [Training]: 100%|██████████| 405/405 [05:11<00:00,  1.30it/s, loss=1.4]  ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/20 - Average Training Loss: 0.0814\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 3/20 [Training]: 100%|██████████| 405/405 [05:10<00:00,  1.30it/s, loss=1.37] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/20 - Average Training Loss: 0.0791\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 4/20 [Training]: 100%|██████████| 405/405 [05:11<00:00,  1.30it/s, loss=1.22] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/20 - Average Training Loss: 0.0775\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 5/20 [Training]: 100%|██████████| 405/405 [05:10<00:00,  1.30it/s, loss=1.04] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/20 - Average Training Loss: 0.0763\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 6/20 [Training]: 100%|██████████| 405/405 [05:09<00:00,  1.31it/s, loss=1.22] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/20 - Average Training Loss: 0.0753\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 7/20 [Training]: 100%|██████████| 405/405 [05:09<00:00,  1.31it/s, loss=1.03] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/20 - Average Training Loss: 0.0746\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 8/20 [Training]: 100%|██████████| 405/405 [05:09<00:00,  1.31it/s, loss=1.14] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/20 - Average Training Loss: 0.0739\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 9/20 [Training]: 100%|██████████| 405/405 [05:09<00:00,  1.31it/s, loss=1.35] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/20 - Average Training Loss: 0.0732\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 10/20 [Training]: 100%|██████████| 405/405 [05:09<00:00,  1.31it/s, loss=1.26] \n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/20 - Average Training Loss: 0.0725\nSaved checkpoint: /kaggle/working/runs/detect/finetune_visdrone_night_scaled_FASTER_RCNN/model_epoch_10.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20 [Training]: 100%|██████████| 405/405 [05:09<00:00,  1.31it/s, loss=1.24] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/20 - Average Training Loss: 0.0705\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 12/20 [Training]: 100%|██████████| 405/405 [05:08<00:00,  1.31it/s, loss=1.12] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/20 - Average Training Loss: 0.0702\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 13/20 [Training]: 100%|██████████| 405/405 [05:09<00:00,  1.31it/s, loss=1.16] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/20 - Average Training Loss: 0.0700\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 14/20 [Training]: 100%|██████████| 405/405 [05:08<00:00,  1.31it/s, loss=1.01] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/20 - Average Training Loss: 0.0699\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 15/20 [Training]: 100%|██████████| 405/405 [05:09<00:00,  1.31it/s, loss=1.04] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15/20 - Average Training Loss: 0.0698\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 16/20 [Training]: 100%|██████████| 405/405 [05:09<00:00,  1.31it/s, loss=0.643]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16/20 - Average Training Loss: 0.0697\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 17/20 [Training]: 100%|██████████| 405/405 [05:08<00:00,  1.31it/s, loss=1.19] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17/20 - Average Training Loss: 0.0697\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 18/20 [Training]: 100%|██████████| 405/405 [05:08<00:00,  1.31it/s, loss=1.4]  ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 18/20 - Average Training Loss: 0.0696\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 19/20 [Training]: 100%|██████████| 405/405 [05:08<00:00,  1.31it/s, loss=1.09] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 19/20 - Average Training Loss: 0.0694\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch 20/20 [Training]: 100%|██████████| 405/405 [05:08<00:00,  1.31it/s, loss=0.997]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20/20 - Average Training Loss: 0.0694\nSaved checkpoint: /kaggle/working/runs/detect/finetune_visdrone_night_scaled_FASTER_RCNN/model_epoch_20.pt\n✅ Training complete.\nFinal model saved to: /kaggle/working/runs/detect/finetune_visdrone_night_scaled_FASTER_RCNN/faster_rcnn_final.pt\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- CELL: EVALUATE mAP ON VALIDATION SET ---\n\n# 1. Install/Upgrade torchmetrics, scipy, and numpy to fix the ValueError\n!pip install -q --upgrade numpy scipy torchmetrics\n\n# --- 🔴 STOP! 🔴 ---\n# After this pip install finishes, you MUST RESTART THE KERNEL.\n# In the Kaggle menu: Click \"Run\" -> \"Restart Session\"\n# After restarting, run this *ENTIRE CELL* again.\n# --------------------\n\nimport os\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import v2 as T\nfrom tqdm import tqdm\n\n# This import will only work AFTER you restart the session\nfrom torchmetrics.detection import MeanAveragePrecision\n\n# --- KAGGLE CHANGE: Add imports and function needed to build the model ---\nfrom torchvision.models.detection import (\n    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n)\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# --- 2. Redefine Dataset and Model Functions ---\n# We must redefine these so this cell can run independently\n\nclass CustomCocoDataset(torchvision.datasets.CocoDetection):\n    def __init__(self, root, annFile, transform=None):\n        super(CustomCocoDataset, self).__init__(root, annFile)\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        img, target_list = super(CustomCocoDataset, self).__getitem__(idx)\n        \n        # 1. Convert PIL Image to Tensor\n        if self.transform is not None:\n            img = self.transform(img)\n\n        # 2. Convert annotation list to the required tensor dictionary\n        target = {}\n        image_id = torch.tensor([self.ids[idx]])\n\n        if not target_list:\n            target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n            target[\"labels\"] = torch.zeros(0, dtype=torch.int64)\n            target[\"image_id\"] = image_id\n            target[\"area\"] = torch.zeros(0, dtype=torch.float32)\n            target[\"iscrowd\"] = torch.zeros(0, dtype=torch.int64)\n            return img, target\n\n        boxes = [obj['bbox'] for obj in target_list]\n        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n        # Convert [x, y, w, h] to [x1, y1, x2, y2]\n        boxes[:, 2:] += boxes[:, :2]\n\n        labels = [obj['category_id'] for obj in target_list]\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        area = torch.as_tensor([obj['area'] for obj in target_list], dtype=torch.float32)\n        iscrowd = torch.as_tensor([obj['iscrowd'] for obj in target_list], dtype=torch.int64)\n\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        return img, target\n\ndef get_faster_rcnn_model(num_classes):\n    model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n# --- 3. Setup Paths and Parameters ---\nMODEL_PATH = '/kaggle/working/runs/detect/finetune_visdrone_night_scaled_FASTER_RCNN/faster_rcnn_final.pt'\nVAL_IMG_DIR = '/kaggle/working/coco_dataset/images/val'\nVAL_ANN_FILE = '/kaggle/working/coco_dataset/annotations/instances_val.json'\nNUM_CLASSES = 11\nBATCH_SIZE = 16\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n\nprint(f\"Using device: {device}\")\n\n# --- 4. Load Validation Data ---\n# CRITICAL: The transform must match the training transform, including resize!\nval_transform = T.Compose([\n    T.ToImage(),\n    T.Resize((256, 256), antialias=True),\n    T.ToDtype(torch.float32, scale=True)\n])\n\ndataset_val = CustomCocoDataset(\n    root=VAL_IMG_DIR,\n    annFile=VAL_ANN_FILE,\n    transform=val_transform\n)\n\ndata_loader_val = DataLoader(\n    dataset_val,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    collate_fn=collate_fn\n)\n\n# --- 5. Load Model ---\nif not os.path.exists(MODEL_PATH):\n    print(f\"❌ ERROR: Model file not found at {MODEL_PATH}\")\nelse:\n    print(\"Loading model for evaluation...\")\n    model = get_faster_rcnn_model(NUM_CLASSES)\n    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n    model.to(device)\n    model.eval()\n    print(\"Model loaded.\")\n\n    # --- 6. Initialize Metric ---\n    metric = MeanAveragePrecision(iou_type=\"bbox\")\n    metric.to(device)\n\n    # --- 7. Evaluation Loop ---\n    print(f\"Running evaluation on {len(dataset_val)} validation images...\")\n    \n    with torch.no_grad():\n        for images, targets in tqdm(data_loader_val, desc=\"Evaluating\"):\n            # Move data to device\n            images = list(img.to(device) for img in images)\n            \n            # Get model predictions\n            # Note: Model output is already in the format torchmetrics expects\n            predictions = model(images) \n            \n            # Format targets for torchmetrics\n            # (must be on CPU for this, then moved to device)\n            targets_formatted = []\n            for t in targets:\n                targets_formatted.append({\n                    \"boxes\": t[\"boxes\"].to(device),\n                    \"labels\": t[\"labels\"].to(device)\n                })\n\n            # Update the metric\n            metric.update(predictions, targets_formatted)\n\n    # --- 8. Compute and Print Final mAP ---\n    print(\"\\nComputing final mAP scores...\")\n    results = metric.compute()\n    \n    print(\"\\n--- FASTER R-CNN mAP RESULTS ---\")\n    print(f\"mAP (IoU=0.50:0.95): {results['map']:.4f}\")\n    print(f\"mAP (IoU=0.50):     {results['map_50']:.4f}\")\n    print(f\"mAP (IoU=0.75):     {results['map_75']:.4f}\")\n    print(\"----------------------------------\")\n    print(f\"mAP (Small):        {results['map_small']:.4f}\")\n    print(f\"mAP (Medium):       {results['map_medium']:.4f}\")\n    print(f\"mAP (Large):        {results['map_large']:.4f}\")\n    print(\"----------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:52:31.749988Z","iopub.execute_input":"2025-11-17T12:52:31.750626Z","iopub.status.idle":"2025-11-17T12:52:35.972973Z","shell.execute_reply.started":"2025-11-17T12:52:31.750593Z","shell.execute_reply":"2025-11-17T12:52:35.971613Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3977923006.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# This import will only work AFTER you restart the session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeanAveragePrecision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# --- KAGGLE CHANGE: Add imports and function needed to build the model ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchmetrics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpackage_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scipy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# back compatibility patch due to SMRMpy using scipy.signal.hamming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/signal/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;31m# mypy: ignore-errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_support_alternative_backends\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_support_alternative_backends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_support_alternative_backends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__all__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/signal/_support_alternative_backends.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_signal_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m   \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_signal_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_delegators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/signal/_signal_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \"\"\"\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_sigtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows\u001b[0m         \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_waveforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m        \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_max_len_seq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmax_len_seq\u001b[0m       \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/signal/windows/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \"\"\"\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_windows\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Deprecated namespaces, to be removed in v2.0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/signal/windows/_windows.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdoccer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfft\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp_fft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_api_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             raise AttributeError(\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/special/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_support_alternative_backends\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_basic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/special/_basic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_specfun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_comb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_comb_int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from ._multiufuncs import (assoc_legendre_p_all,\n\u001b[0m\u001b[1;32m     23\u001b[0m                            legendre_p_all)\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/special/_multiufuncs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m sph_legendre_p = MultiUFunc(\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0msph_legendre_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     r\"\"\"sph_legendre_p(n, m, theta, *, diff_n=0)\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/special/_multiufuncs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ufunc_or_ufuncs, doc, force_complex_output, **default_kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mufunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mufuncs_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mufunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     raise ValueError(\"All ufuncs must have type `numpy.ufunc`.\"\n\u001b[0m\u001b[1;32m     42\u001b[0m                                      f\" Received {ufunc_or_ufuncs}\")\n\u001b[1;32m     43\u001b[0m                 \u001b[0mseen_input_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"->\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: All ufuncs must have type `numpy.ufunc`. Received (<ufunc 'sph_legendre_p'>, <ufunc 'sph_legendre_p'>, <ufunc 'sph_legendre_p'>)"],"ename":"ValueError","evalue":"All ufuncs must have type `numpy.ufunc`. Received (<ufunc 'sph_legendre_p'>, <ufunc 'sph_legendre_p'>, <ufunc 'sph_legendre_p'>)","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"import os\n\nprint(\"--- Training Results (Faster R-CNN) ---\")\n\n# --- KAGGLE CHANGE ---\n# This path matches the 'OUTPUT_DIR' from your train_faster_rcnn.py script\nresults_dir = '/kaggle/working/runs/detect/finetune_visdrone_night_scaled_FASTER_RCNN'\n\nif os.path.exists(results_dir):\n    print(f\"Results are saved in: {results_dir}\")\n\n    # List all files in the directory (checkpoints and final model)\n    !ls -lh {results_dir}\n\n    print(\"\\n--- Training Metrics ---\")\n    print(\"Training loss was printed to the console after each epoch.\")\n    print(\"Note: A 'results.png' is not automatically generated for this type of training.\")\n    print(\"To get validation mAP, a separate evaluation script must be run.\")\n\n    # Point to the final model\n    final_model_path = os.path.join(results_dir, 'faster_rcnn_final.pt')\n    if os.path.exists(final_model_path):\n        print(f\"\\nYour final fine-tuned model is at: {final_model_path}\")\n    else:\n        print(\"\\nFinal model file 'faster_rcnn_final.pt' not found. Training may still be running or encountered an error.\")\nelse:\n    print(f\"❌ ERROR: Results directory not found at {results_dir}\")\n    print(\"Please ensure the 'train_faster_rcnn.py' script ran successfully.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"626L8BYsBhpK","outputId":"12c6d348-12df-4b7b-d1df-769d1c88342b","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:33:59.851920Z","iopub.execute_input":"2025-11-17T12:33:59.853019Z","iopub.status.idle":"2025-11-17T12:34:00.015349Z","shell.execute_reply.started":"2025-11-17T12:33:59.852984Z","shell.execute_reply":"2025-11-17T12:34:00.014435Z"}},"outputs":[{"name":"stdout","text":"--- Training Results (Faster R-CNN) ---\nResults are saved in: /kaggle/working/runs/detect/finetune_visdrone_night_scaled_FASTER_RCNN\ntotal 219M\n-rw-r--r-- 1 root root 73M Nov 17 12:33 faster_rcnn_final.pt\n-rw-r--r-- 1 root root 73M Nov 17 11:41 model_epoch_10.pt\n-rw-r--r-- 1 root root 73M Nov 17 12:33 model_epoch_20.pt\n\n--- Training Metrics ---\nTraining loss was printed to the console after each epoch.\nNote: A 'results.png' is not automatically generated for this type of training.\nTo get validation mAP, a separate evaluation script must be run.\n\nYour final fine-tuned model is at: /kaggle/working/runs/detect/finetune_visdrone_night_scaled_FASTER_RCNN/faster_rcnn_final.pt\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport torch\nimport glob\nfrom PIL import Image, ImageDraw\nfrom torchvision.transforms import v2 as T\n# from IPython.display import Image as IpyImage, display # Not needed in Kaggle script\n\n# --- KAGGLE CHANGE: Add imports and function needed to build the model ---\nfrom torchvision.models.detection import (\n    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n)\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\ndef get_faster_rcnn_model(num_classes):\n    \"\"\"\n    Loads a pre-trained Faster R-CNN model and replaces the classifier head.\n    \"\"\"\n    model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT)\n    print(\"Using lightweight MobileNetV3-Large backbone.\")\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n# --- End of added function ---\n\n\n# --- 1. Load your newly fine-tuned model ---\n# --- KAGGLE CHANGE ---\nMODEL_DIR = '/kaggle/working/runs/detect/finetune_visdrone_night_scaled_FASTER_RCNN'\nMODEL_PATH = os.path.join(MODEL_DIR, 'faster_rcnn_final.pt')\nNUM_CLASSES = 11 # 10 VisDrone classes + 1 background\nCONF_THRESHOLD = 0.25\n\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n\nif not os.path.exists(MODEL_PATH):\n    print(f\"❌ ERROR: Model file not found at {MODEL_PATH}\")\n    print(\"Please ensure the training cell ran successfully.\")\nelse:\n    print(f\"Loading model from: {MODEL_PATH}\")\n    model = get_faster_rcnn_model(NUM_CLASSES)\n    if model:\n        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n        model.to(device)\n        model.eval() # Set model to evaluation mode\n\n        # --- 2. Get a random image from your validation set ---\n        # --- KAGGLE CHANGE ---\n        val_images = glob.glob('/kaggle/working/coco_dataset/images/val/*.jpg')\n        if val_images:\n            test_image_path = val_images[0]\n            print(f\"Running inference on: {test_image_path}\")\n\n            # Load the image\n            img_pil = Image.open(test_image_path).convert('RGB')\n\n            # --- KAGGLE CHANGE: CRITICAL FIX ---\n            # The transform MUST match the training transform, including the resize.\n            transform = T.Compose([\n                T.ToImage(),\n                T.Resize((256, 256), antialias=True), # <-- This was missing\n                T.ToDtype(torch.float32, scale=True)\n            ])\n            img_tensor = transform(img_pil)\n\n            # Add a batch dimension and send to device\n            input_batch = [img_tensor.to(device)]\n\n            # --- 3. Run prediction ---\n            with torch.no_grad():\n                predictions = model(input_batch)\n\n            # predictions[0] contains the results for our single image\n            pred = predictions[0]\n\n            # --- 4. Draw results and display ---\n            print(\"\\n--- Detection Result (Boxes Only) ---\")\n\n            # Get boxes and scores that meet the confidence threshold\n            high_conf_indices = pred['scores'] > CONF_THRESHOLD\n            boxes = pred['boxes'][high_conf_indices].cpu().numpy()\n            \n            # --- KAGGLE CHANGE: Draw on the *original* un-resized image ---\n            # We must scale the 256x256 boxes back to the original image size\n            orig_w, orig_h = img_pil.size\n            # The model output boxes are for a 256x256 image\n            scale_x = orig_w / 256.0\n            scale_y = orig_h / 256.0\n            \n            scaled_boxes = []\n            for box in boxes:\n                x1, y1, x2, y2 = box\n                scaled_boxes.append([\n                    x1 * scale_x, \n                    y1 * scale_y, \n                    x2 * scale_x, \n                    y2 * scale_y\n                ])\n\n            # Draw the *scaled* boxes on the *original* PIL image\n            draw = ImageDraw.Draw(img_pil)\n            for box in scaled_boxes:\n                # 'box' is [x1, y1, x2, y2]\n                draw.rectangle(list(box), outline=\"red\", width=3)\n\n            # Save the resulting image\n            base_filename = os.path.basename(test_image_path)\n            save_path = os.path.join(MODEL_DIR, f\"detection_{base_filename}\")\n            img_pil.save(save_path)\n\n            print(f\"Result saved to: {save_path}\")\n            print(\"You can view the saved image in the Kaggle output viewer.\")\n\n            # --- KAGGLE CHANGE ---\n            # Removed 'display(IpyImage(filename=save_path))'\n            # Kaggle will automatically show images saved to /kaggle/working/\n\n        else:\n            print(f\"No validation images found in /kaggle/working/coco_dataset/images/val/\")\n    else:\n        print(\"Model could not be loaded. Skipping inference.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lOscgO8JB-4o","outputId":"29a1f96f-5d74-43ae-f8d9-82da0ef6a4f2","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:34:08.262455Z","iopub.execute_input":"2025-11-17T12:34:08.262794Z","iopub.status.idle":"2025-11-17T12:34:08.860639Z","shell.execute_reply.started":"2025-11-17T12:34:08.262762Z","shell.execute_reply":"2025-11-17T12:34:08.860001Z"}},"outputs":[{"name":"stdout","text":"Loading model from: /kaggle/working/runs/detect/finetune_visdrone_night_scaled_FASTER_RCNN/faster_rcnn_final.pt\nUsing lightweight MobileNetV3-Large backbone.\nRunning inference on: /kaggle/working/coco_dataset/images/val/0000287_02801_d_0000773.jpg\n\n--- Detection Result (Boxes Only) ---\nResult saved to: /kaggle/working/runs/detect/finetune_visdrone_night_scaled_FASTER_RCNN/detection_0000287_02801_d_0000773.jpg\nYou can view the saved image in the Kaggle output viewer.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport torch\nimport glob\nfrom PIL import Image, ImageDraw, ImageFont\nfrom torchvision.transforms import v2 as T\nfrom tqdm import tqdm\n\n# --- KAGGLE CHANGE: Add imports and function needed to build the model ---\nfrom torchvision.models.detection import (\n    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n)\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\ndef get_faster_rcnn_model(num_classes):\n    \"\"\"\n    Loads a pre-trained Faster R-CNN model and replaces the classifier head.\n    \"\"\"\n    model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT)\n    # print(\"Using lightweight MobileNetV3-Large backbone.\") # Silenced for less output\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n# --- End of added function ---\n\n\n# We need the class names from our 'convert_to_coco.py' script to draw labels\nVISDRONE_CATEGORIES = [\n    {\"id\": 1, \"name\": \"pedestrian\"}, {\"id\": 2, \"name\": \"person\"},\n    {\"id\": 3, \"name\": \"bicycle\"}, {\"id\": 4, \"name\": \"car\"},\n    {\"id\": 5, \"name\": \"van\"}, {\"id\": 6, \"name\": \"truck\"},\n    {\"id\": 7, \"name\": \"tricycle\"}, {\"id\": 8, \"name\": \"awning-tricycle\"},\n    {\"id\": 9, \"name\": \"bus\"}, {\"id\": 10, \"name\": \"motor\"},\n]\n# Create a mapping from ID (1-10) to name\nCOCO_CATEGORY_MAP = {cat['id']: cat['name'] for cat in VISDRONE_CATEGORIES}\n\n\n# --- 2. Main Inference Function ---\ndef run_inference_on_all_val():\n    print(\"Loading fine-tuned model...\")\n    # --- Paths and Parameters ---\n    # --- KAGGLE CHANGE ---\n    MODEL_DIR = '/kaggle/working/runs/detect/finetune_visdrone_night_scaled_FASTER_RCNN'\n    MODEL_PATH = os.path.join(MODEL_DIR, 'faster_rcnn_final.pt')\n    NUM_CLASSES = 11 # 10 VisDrone classes + 1 background\n\n    # Equivalent to your YOLO script's parameters\n    CONF_THRESHOLD = 0.25\n    # --- KAGGLE CHANGE ---\n    VAL_IMAGES_PATH = '/kaggle/working/coco_dataset/images/val/' # Use the COCO dataset path\n\n    # Equivalent to project='runs/detect', name='predictions_with_labels'\n    # --- KAGGLE CHANGE ---\n    OUTPUT_DIR = '/kaggle/working/runs/detect/predictions_with_labels_FASTER_RCNN'\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n\n    if not os.path.exists(MODEL_PATH) or not get_faster_rcnn_model:\n        print(f\"❌ ERROR: Model file not found at {MODEL_PATH} or 'get_faster_rcnn_model' not available.\")\n        print(\"Please ensure the training cell ran successfully.\")\n        return\n\n    # --- Load Model ---\n    model = get_faster_rcnn_model(NUM_CLASSES)\n    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n    model.to(device)\n    model.eval() # Set model to evaluation mode\n    print(\"Model loaded.\")\n\n    # --- Load Font for drawing labels ---\n    try:\n        # Use a default font that's available in PIL\n        font = ImageFont.load_default()\n    except IOError:\n        print(\"Cannot load default font. Using dummy font.\")\n        font = None\n\n    # --- Get all image paths ---\n    val_images_paths = glob.glob(os.path.join(VAL_IMAGES_PATH, '*.jpg'))\n    if not val_images_paths:\n        print(f\"No images found in {VAL_IMAGES_PATH}\")\n        return\n\n    print(f\"Running predictions on all {len(val_images_paths)} images in: {VAL_IMAGES_PATH}\")\n\n    # --- KAGGLE CHANGE: CRITICAL FIX ---\n    # The transform MUST match the training transform, including the resize.\n    transform = T.Compose([\n        T.ToImage(),\n        T.Resize((256, 256), antialias=True), # <-- This was missing\n        T.ToDtype(torch.float32, scale=True)\n    ])\n\n    # --- Loop and predict (equivalent to model.predict) ---\n    for img_path in tqdm(val_images_paths):\n        # Load and transform the image\n        img_pil = Image.open(img_path).convert('RGB')\n        \n        # --- KAGGLE CHANGE: Store original size for scaling boxes ---\n        orig_w, orig_h = img_pil.size\n        \n        img_tensor = transform(img_pil) # This creates a 256x256 tensor\n        input_batch = [img_tensor.to(device)]\n\n        # Run prediction\n        with torch.no_grad():\n            predictions = model(input_batch)\n\n        pred = predictions[0]\n\n        # Filter boxes, scores, and labels by confidence\n        high_conf_indices = pred['scores'] > CONF_THRESHOLD\n        boxes = pred['boxes'][high_conf_indices].cpu().numpy()\n        labels = pred['labels'][high_conf_indices].cpu().numpy()\n        scores = pred['scores'][high_conf_indices].cpu().numpy()\n        \n        # --- KAGGLE CHANGE: Add scaling factors ---\n        # The model output boxes are for a 256x256 image\n        scale_x = orig_w / 256.0\n        scale_y = orig_h / 256.0\n\n        # Draw results on the image\n        draw = ImageDraw.Draw(img_pil)\n        for box, label_id, score in zip(boxes, labels, scores):\n            \n            # --- KAGGLE CHANGE: Scale the box ---\n            x1, y1, x2, y2 = box\n            scaled_box = [\n                x1 * scale_x, \n                y1 * scale_y, \n                x2 * scale_x, \n                y2 * scale_y\n            ]\n            \n            # Draw the *scaled* box\n            draw.rectangle(list(scaled_box), outline=\"red\", width=3)\n\n            # Get class name (show_labels=True)\n            class_name = COCO_CATEGORY_MAP.get(label_id, f'ID: {label_id}')\n\n            # Create text (show_conf=True)\n            text = f\"{class_name} {score:.2f}\"\n\n            # Draw text above the *scaled* box\n            text_position = (scaled_box[0], scaled_box[1] - 10)\n            draw.text(text_position, text, fill=\"red\", font=font)\n\n        # Save the resulting image (save=True)\n        base_filename = os.path.basename(img_path)\n        save_path = os.path.join(OUTPUT_DIR, base_filename)\n        img_pil.save(save_path)\n\n    print(\"\\n✅ Complete. Predictions with labels saved to:\")\n    print(OUTPUT_DIR)\n    print(\"You can view the saved images in the Kaggle output viewer.\")\n\n# --- Run the main function ---\nif __name__ == \"__main__\":\n    run_inference_on_all_val()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"t7YagNNXE_Xn","outputId":"1ff84d45-e3eb-4921-b313-d26229b682fe","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:34:50.417775Z","iopub.execute_input":"2025-11-17T12:34:50.418413Z","iopub.status.idle":"2025-11-17T12:35:16.132728Z","shell.execute_reply.started":"2025-11-17T12:34:50.418386Z","shell.execute_reply":"2025-11-17T12:35:16.132126Z"}},"outputs":[{"name":"stdout","text":"Loading fine-tuned model...\nModel loaded.\nRunning predictions on all 548 images in: /kaggle/working/coco_dataset/images/val/\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 548/548 [00:25<00:00, 21.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n✅ Complete. Predictions with labels saved to:\n/kaggle/working/runs/detect/predictions_with_labels_FASTER_RCNN\nYou can view the saved images in the Kaggle output viewer.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport torch\nimport glob\nfrom PIL import Image, ImageDraw, ImageFont\nfrom torchvision.transforms import v2 as T\nfrom tqdm import tqdm\n\n# --- KAGGLE CHANGE: Add imports and function needed to build the model ---\nfrom torchvision.models.detection import (\n    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n)\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\ndef get_faster_rcnn_model(num_classes):\n    \"\"\"\n    Loads a pre-trained Faster R-CNN model and replaces the classifier head.\n    \"\"\"\n    model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT)\n    # print(\"Using lightweight MobileNetV3-Large backbone.\") # Silenced for less output\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n# --- End of added function ---\n\n\n# NOTE: We don't need VISDRONE_CATEGORIES since show_labels=False\n\n# --- 2. Main Inference Function ---\ndef run_inference_on_all_val_boxes_only():\n    print(\"Loading fine-tuned model...\")\n    # --- Paths and Parameters ---\n    # --- KAGGLE CHANGE ---\n    MODEL_DIR = '/kaggle/working/runs/detect/finetune_visdrone_night_scaled_FASTER_RCNN'\n    MODEL_PATH = os.path.join(MODEL_DIR, 'faster_rcnn_final.pt')\n    NUM_CLASSES = 11 # 10 VisDrone classes + 1 background\n\n    # Equivalent to your YOLO script's parameters\n    CONF_THRESHOLD = 0.25\n    # --- KAGGLE CHANGE ---\n    VAL_IMAGES_PATH = '/kaggle/working/coco_dataset/images/val/' # Use the COCO dataset path\n\n    # Equivalent to project='runs/detect', name='predictions_boxes_only'\n    # --- KAGGLE CHANGE ---\n    OUTPUT_DIR = '/kaggle/working/runs/detect/predictions_boxes_only_FASTER_RCNN'\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n\n    if not os.path.exists(MODEL_PATH) or not get_faster_rcnn_model:\n        print(f\"❌ ERROR: Model file not found at {MODEL_PATH} or 'get_faster_rcnn_model' not available.\")\n        print(\"Please ensure the training cell ran successfully.\")\n        return\n\n    # --- Load Model ---\n    model = get_faster_rcnn_model(NUM_CLASSES)\n    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n    model.to(device)\n    model.eval() # Set model to evaluation mode\n    print(\"Model loaded.\")\n\n    # --- Get all image paths ---\n    val_images_paths = glob.glob(os.path.join(VAL_IMAGES_PATH, '*.jpg'))\n    if not val_images_paths:\n        print(f\"No images found in {VAL_IMAGES_PATH}\")\n        return\n\n    print(f\"Running predictions (boxes only) on all {len(val_images_paths)} images in: {VAL_IMAGES_PATH}\")\n\n    # --- KAGGLE CHANGE: CRITICAL FIX ---\n    # The transform MUST match the training transform, including the resize.\n    transform = T.Compose([\n        T.ToImage(),\n        T.Resize((256, 256), antialias=True), # <-- This was missing\n        T.ToDtype(torch.float32, scale=True)\n    ])\n\n    # --- Loop and predict (equivalent to model.predict) ---\n    for img_path in tqdm(val_images_paths):\n        # Load and transform the image\n        img_pil = Image.open(img_path).convert('RGB')\n        \n        # --- KAGGLE CHANGE: Store original size for scaling boxes ---\n        orig_w, orig_h = img_pil.size\n        \n        img_tensor = transform(img_pil) # This creates a 256x256 tensor\n        input_batch = [img_tensor.to(device)]\n\n        # Run prediction\n        with torch.no_grad():\n            predictions = model(input_batch)\n\n        pred = predictions[0]\n\n        # Filter boxes by confidence\n        high_conf_indices = pred['scores'] > CONF_THRESHOLD\n        boxes = pred['boxes'][high_conf_indices].cpu().numpy()\n        # No need to get labels or scores since we aren't drawing them\n        \n        # --- KAGGLE CHANGE: Add scaling factors ---\n        # The model output boxes are for a 256x256 image\n        scale_x = orig_w / 256.0\n        scale_y = orig_h / 256.0\n\n        # Draw results on the image\n        draw = ImageDraw.Draw(img_pil)\n        for box in boxes:\n            \n            # --- KAGGLE CHANGE: Scale the box ---\n            x1, y1, x2, y2 = box\n            scaled_box = [\n                x1 * scale_x, \n                y1 * scale_y, \n                x2 * scale_x, \n                y2 * scale_y\n            ]\n\n            # Draw the *scaled* box (show_labels=False, show_conf=False)\n            draw.rectangle(list(scaled_box), outline=\"red\", width=3)\n\n            # --- All label/confidence drawing code is removed ---\n\n        # Save the resulting image (save=True)\n        base_filename = os.path.basename(img_path)\n        save_path = os.path.join(OUTPUT_DIR, base_filename)\n        img_pil.save(save_path)\n\n    print(\"\\n✅ Complete. Predictions (boxes only) saved to:\")\n    print(OUTPUT_DIR)\n    print(\"You can view the saved images in the Kaggle output viewer.\")\n\n# --- Run the main function ---\nif __name__ == \"__main__\":\n    run_inference_on_all_val_boxes_only()","metadata":{"id":"zYxkPpkIFGfB","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:35:44.441245Z","iopub.execute_input":"2025-11-17T12:35:44.442037Z","iopub.status.idle":"2025-11-17T12:36:00.169972Z","shell.execute_reply.started":"2025-11-17T12:35:44.442009Z","shell.execute_reply":"2025-11-17T12:36:00.169171Z"}},"outputs":[{"name":"stdout","text":"Loading fine-tuned model...\nModel loaded.\nRunning predictions (boxes only) on all 548 images in: /kaggle/working/coco_dataset/images/val/\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 548/548 [00:15<00:00, 35.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n✅ Complete. Predictions (boxes only) saved to:\n/kaggle/working/runs/detect/predictions_boxes_only_FASTER_RCNN\nYou can view the saved images in the Kaggle output viewer.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import shutil\nimport os\n\n# --- KAGGLE CHANGE ---\n# All paths must point to /kaggle/working/\nbase_runs_dir = '/kaggle/working/runs/detect'\n\n# --- KAGGLE CHANGE ---\n# All code related to 'drive_save_path' has been removed.\n# Zip files will be created in /kaggle/working/\n# You can download them from the \"Output\" tab in the Kaggle UI.\n\n# --- 1. Zip the Main Training Run (includes .pt model files) ---\nfolder_to_zip = 'finetune_visdrone_night_scaled_FASTER_RCNN'\nzip_name = 'finetune_visdrone_night_scaled_FASTER_RCNN_RUN'\nfull_folder_path = os.path.join(base_runs_dir, folder_to_zip)\n\nif os.path.exists(full_folder_path):\n    print(f\"Zipping {folder_to_zip}...\")\n    shutil.make_archive(\n        zip_name,\n        'zip',\n        root_dir=base_runs_dir,\n        base_dir=folder_to_zip\n    )\n    print(f\"✅ Created {zip_name}.zip in /kaggle/working/\")\n\n    # --- KAGGLE CHANGE ---\n    # Removed 'Move to Drive' block\nelse:\n    print(f\"⚠️ Warning: Folder not found, skipping zip: {full_folder_path}\")\n\n\n# --- 2. Zip the Predictions (With Labels) ---\nfolder_to_zip = 'predictions_with_labels_FASTER_RCNN'\nzip_name_labels = 'predictions_with_labels_FASTER_RCNN_IMAGES'\nfull_folder_path = os.path.join(base_runs_dir, folder_to_zip)\n\nif os.path.exists(full_folder_path):\n    print(f\"\\nZipping {folder_to_zip}...\")\n    shutil.make_archive(\n        zip_name_labels,\n        'zip',\n        root_dir=base_runs_dir,\n        base_dir=folder_to_zip\n    )\n    print(f\"✅ Created {zip_name_labels}.zip in /kaggle/working/\")\n\n    # --- KAGGLE CHANGE ---\n    # Removed 'Move to Drive' block\nelse:\n    print(f\"⚠️ Warning: Folder not found, skipping zip: {full_folder_path}\")\n\n\n# --- 3. Zip the Predictions (Boxes Only) ---\nfolder_to_zip = 'predictions_boxes_only_FASTER_RCNN'\nzip_name_boxes = 'predictions_boxes_only_FASTER_RCNN_IMAGES'\nfull_folder_path = os.path.join(base_runs_dir, folder_to_zip)\n\nif os.path.exists(full_folder_path):\n    print(f\"\\nZipping {folder_to_zip}...\")\n    shutil.make_archive(\n        zip_name_boxes,\n        'zip',\n        root_dir=base_runs_dir,\n        base_dir=folder_to_zip\n    )\n    print(f\"✅ Created {zip_name_boxes}.zip in /kaggle/working/\")\n\n    # --- KAGGLE CHANGE ---\n    # Removed 'Move to Drive' block\nelse:\n    print(f\"⚠️ Warning: Folder not found, skipping zip: {full_folder_path}\")\n\n\nprint(\"---\" * 20)\nprint(\"✅ ALL DONE. Your Faster R-CNN training run, model, and prediction images have been saved to /kaggle/working/.\")\nprint(\"You can download the .zip files from the 'Output' section in the Kaggle UI.\")\nprint(\"---\" * 20)","metadata":{"id":"jbH37dT-4k7V","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:36:06.798931Z","iopub.execute_input":"2025-11-17T12:36:06.799216Z","iopub.status.idle":"2025-11-17T12:36:18.342432Z","shell.execute_reply.started":"2025-11-17T12:36:06.799194Z","shell.execute_reply":"2025-11-17T12:36:18.341664Z"}},"outputs":[{"name":"stdout","text":"Zipping finetune_visdrone_night_scaled_FASTER_RCNN...\n✅ Created finetune_visdrone_night_scaled_FASTER_RCNN_RUN.zip in /kaggle/working/\n\nZipping predictions_with_labels_FASTER_RCNN...\n✅ Created predictions_with_labels_FASTER_RCNN_IMAGES.zip in /kaggle/working/\n\nZipping predictions_boxes_only_FASTER_RCNN...\n✅ Created predictions_boxes_only_FASTER_RCNN_IMAGES.zip in /kaggle/working/\n------------------------------------------------------------\n✅ ALL DONE. Your Faster R-CNN training run, model, and prediction images have been saved to /kaggle/working/.\nYou can download the .zip files from the 'Output' section in the Kaggle UI.\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# --- CELL: EVALUATE mAP@[.5:.05:.95] (MANUAL PYTHON/TORCH) ---\n\nimport os\nimport torch\nimport torchvision\nimport json\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import v2 as T\nfrom tqdm import tqdm\nimport numpy as np\n\n# --- 1. Redefine Dataset and Model Functions ---\n# (Must be redefined in case the session was reset or to run independently)\n\nfrom torchvision.models.detection import (\n    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n)\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nclass CustomCocoDataset(torchvision.datasets.CocoDetection):\n    def __init__(self, root, annFile, transform=None):\n        super(CustomCocoDataset, self).__init__(root, annFile)\n        self.transform = transform\n        # We also need to load the raw COCO ground truth\n        from pycocotools.coco import COCO\n        self.coco = COCO(annFile)\n        # Fix for the 'info' key error\n        if 'info' not in self.coco.dataset:\n            self.coco.dataset['info'] = {}\n\n    def __getitem__(self, idx):\n        img, target_list = super(CustomCocoDataset, self).__getitem__(idx)\n        \n        if self.transform is not None:\n            img = self.transform(img)\n\n        target = {}\n        image_id = self.ids[idx]\n        target[\"image_id\"] = torch.tensor([image_id])\n        \n        # Get original image info\n        img_info = self.coco.loadImgs(image_id)[0]\n        target[\"orig_w\"] = img_info['width']\n        target[\"orig_h\"] = img_info['height']\n        \n        # Get ground truth boxes\n        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n        anns = self.coco.loadAnns(ann_ids)\n        \n        gt_boxes = []\n        gt_labels = []\n        for ann in anns:\n            # Skip 'iscrowd' annotations\n            if ann.get('iscrowd', 0) == 1:\n                continue\n            # VisDrone class 0 and 11 are ignored\n            if ann['category_id'] in [0, 11]:\n                continue\n            \n            x, y, w, h = ann['bbox']\n            gt_boxes.append([x, y, x + w, y + h]) # to [x1, y1, x2, y2]\n            gt_labels.append(ann['category_id'])\n\n        if not gt_boxes:\n            target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n            target[\"labels\"] = torch.zeros(0, dtype=torch.int64)\n        else:\n            target[\"boxes\"] = torch.as_tensor(gt_boxes, dtype=torch.float32)\n            target[\"labels\"] = torch.as_tensor(gt_labels, dtype=torch.int64)\n\n        return img, target\n\ndef get_faster_rcnn_model(num_classes):\n    model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n# --- 2. Define IoU Helper Function ---\ndef calculate_iou(box1, box2):\n    \"\"\"Calculates IoU for two [x1, y1, x2, y2] boxes\"\"\"\n    x1_inter = max(box1[0], box2[0])\n    y1_inter = max(box1[1], box2[1])\n    x2_inter = min(box1[2], box2[2])\n    y2_inter = min(box1[3], box2[3])\n\n    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n    \n    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    \n    union_area = box1_area + box2_area - inter_area\n    \n    if union_area == 0:\n        return 0.0\n    return inter_area / union_area\n\n# --- 3. Setup Paths and Parameters ---\nMODEL_PATH = '/kaggle/working/runs/detect/finetune_visdrone_night_scaled_FASTER_RCNN/faster_rcnn_final.pt'\nVAL_IMG_DIR = '/kaggle/working/coco_dataset/images/val'\nVAL_ANN_FILE = '/kaggle/working/coco_dataset/annotations/instances_val.json'\nNUM_CLASSES = 11 # 10 classes + 1 background\nBATCH_SIZE = 16\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n\n# This is [0.5, 0.55, 0.6, ..., 0.90, 0.95]\nIOU_THRESHOLDS = np.linspace(0.5, 0.95, 10)\nprint(f\"Calculating mAP for 10 IoU thresholds: {IOU_THRESHOLDS}\")\nprint(f\"Using device: {device}\")\n\n# --- 4. Load Data and Model ---\nval_transform = T.Compose([\n    T.ToImage(), T.Resize((256, 256), antialias=True), T.ToDtype(torch.float32, scale=True)\n])\ndataset_val = CustomCocoDataset(root=VAL_IMG_DIR, annFile=VAL_ANN_FILE, transform=val_transform)\ndata_loader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate_fn)\n\nprint(\"Loading model for evaluation...\")\nmodel = get_faster_rcnn_model(NUM_CLASSES)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=device))\nmodel.to(device)\nmodel.eval()\nprint(\"Model loaded.\")\n\n# --- 5. Evaluation Loop (Run Once) ---\nall_preds = [] # List of [image_idx, class_id, score, x1, y1, x2, y2]\nall_gts = []   # List of [image_idx, class_id, x1, y1, x2, y2]\n\nprint(f\"Running evaluation on {len(dataset_val)} validation images...\")\n\nwith torch.no_grad():\n    for batch_idx, (images, targets) in enumerate(tqdm(data_loader_val, desc=\"Evaluating\")):\n        images = list(img.to(device) for img in images)\n        predictions = model(images)\n        \n        for i, (pred, target) in enumerate(zip(predictions, targets)):\n            image_idx = target[\"image_id\"].item()\n            \n            # --- Store Ground Truths ---\n            for gt_box, gt_label in zip(target['boxes'], target['labels']):\n                all_gts.append([image_idx, gt_label.item()] + gt_box.tolist())\n\n            # --- Store Predictions ---\n            orig_w, orig_h = target['orig_w'], target['orig_h']\n            scale_x, scale_y = orig_w / 256.0, orig_h / 256.0\n            \n            for box, label, score in zip(pred['boxes'], pred['labels'], pred['scores']):\n                x1, y1, x2, y2 = box.tolist()\n                scaled_box = [x1 * scale_x, y1 * scale_y, x2 * scale_x, y2 * scale_y]\n                all_preds.append([image_idx, label.item(), score.item()] + scaled_box)\n\nprint(\"Evaluation loop complete. Calculating mAP...\")\n\n# --- 6. Calculate mAP for all IoU Thresholds ---\nall_map_scores = [] # Store the mAP for each IoU threshold\n\nfor iou_threshold in IOU_THRESHOLDS:\n    average_precisions = []\n    # VisDrone categories are 1 through 10\n    for class_id in range(1, 11): \n        \n        class_preds = sorted([p for p in all_preds if p[1] == class_id], key=lambda x: x[2], reverse=True)\n        class_gts = [g for g in all_gts if g[1] == class_id]\n        \n        total_gts_for_class = len(class_gts)\n        if total_gts_for_class == 0:\n            continue \n\n        # --- 🔴 LOGIC FIX HERE ---\n        # Create a dictionary to track detected GTs *per image*\n        # Key: image_idx, Value: list of bools (one for each GT box *in that image*)\n        gt_detected_map = {}\n        for g in class_gts:\n            img_idx = g[0]\n            if img_idx not in gt_detected_map:\n                # Count how many GTs for this class are in this image\n                gts_in_img = len([gt for gt in class_gts if gt[0] == img_idx])\n                gt_detected_map[img_idx] = [False] * gts_in_img\n        # --- End of Fix ---\n\n        true_positives = np.zeros(len(class_preds))\n        false_positives = np.zeros(len(class_preds))\n        \n        for pred_idx, pred in enumerate(class_preds):\n            img_idx = pred[0]\n            pred_box = pred[3:]\n            \n            # --- 🔴 LOGIC FIX HERE ---\n            # Get only GTs for this class *in this image*\n            # Store them with their *local index* for this image\n            gts_for_this_image = [g[2:] for g in class_gts if g[0] == img_idx]\n            gt_boxes_for_img = list(enumerate(gts_for_this_image))\n            # --- End of Fix ---\n            \n            if not gt_boxes_for_img:\n                false_positives[pred_idx] = 1\n                continue\n\n            best_iou = 0\n            best_gt_local_idx = -1 # This is the index *within the image's list*\n            \n            # Find the best matching ground truth box\n            for local_idx, gt_box in gt_boxes_for_img:\n                iou = calculate_iou(pred_box, gt_box)\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_local_idx = local_idx # This is now the correct LOCAL index (0, 1, ...)\n            \n            if best_iou >= iou_threshold:\n                # --- 🔴 LOGIC FIX HERE ---\n                # Use the local_idx to check the detection map\n                if not gt_detected_map[img_idx][best_gt_local_idx]:\n                    true_positives[pred_idx] = 1\n                    gt_detected_map[img_idx][best_gt_local_idx] = True\n                else:\n                    false_positives[pred_idx] = 1\n                # --- End of Fix ---\n            else:\n                false_positives[pred_idx] = 1\n                \n        # Calculate precision and recall\n        tp_cumsum = np.cumsum(true_positives)\n        fp_cumsum = np.cumsum(false_positives)\n        \n        precision = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-16)\n        recall = tp_cumsum / (total_gts_for_class + 1e-16)\n        \n        # Calculate Average Precision (AP)\n        precision = np.concatenate(([0.], precision, [0.]))\n        recall = np.concatenate(([0.], recall, [1.]))\n        \n        for i in range(len(precision) - 2, -1, -1):\n            precision[i] = max(precision[i], precision[i + 1])\n            \n        recall_changes = np.where(recall[1:] != recall[:-1])[0] + 1\n        \n        ap = np.sum((recall[recall_changes] - recall[recall_changes - 1]) * precision[recall_changes])\n        average_precisions.append(ap)\n    \n    # Store the mAP for this IoU threshold\n    map_at_iou = np.mean(average_precisions)\n    all_map_scores.append(map_at_iou)\n    # Print the result for this threshold\n    print(f\"  -> mAP @ IoU={iou_threshold:.2f} = {map_at_iou:.4f}\")\n\n\n# --- 7. Print Final Results ---\nfinal_map = np.mean(all_map_scores)\n\nprint(\"\\n--- FASTER R-CNN mAP RESULTS (Manual) ---\")\nprint(f\"mAP (IoU=0.50): {all_map_scores[0]:.4f}\")\nprint(f\"mAP (IoU=0.75): {all_map_scores[5]:.4f}\")\nprint(\"-------------------------------------------\")\nprint(f\"mAP@[.5:.05:.95]: {final_map:.4f}\")\nprint(\"-------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:20:42.754053Z","iopub.execute_input":"2025-11-17T13:20:42.754901Z","iopub.status.idle":"2025-11-17T13:25:25.800512Z","shell.execute_reply.started":"2025-11-17T13:20:42.754870Z","shell.execute_reply":"2025-11-17T13:25:25.799705Z"}},"outputs":[{"name":"stdout","text":"Calculating mAP for 10 IoU thresholds: [        0.5        0.55         0.6        0.65         0.7        0.75         0.8        0.85         0.9        0.95]\nUsing device: cuda:0\nloading annotations into memory...\nDone (t=0.10s)\ncreating index...\nindex created!\nloading annotations into memory...\nDone (t=0.43s)\ncreating index...\nindex created!\nLoading model for evaluation...\nModel loaded.\nRunning evaluation on 548 validation images...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 35/35 [00:16<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Evaluation loop complete. Calculating mAP...\n  -> mAP @ IoU=0.50 = 0.0387\n  -> mAP @ IoU=0.55 = 0.0339\n  -> mAP @ IoU=0.60 = 0.0288\n  -> mAP @ IoU=0.65 = 0.0227\n  -> mAP @ IoU=0.70 = 0.0183\n  -> mAP @ IoU=0.75 = 0.0126\n  -> mAP @ IoU=0.80 = 0.0078\n  -> mAP @ IoU=0.85 = 0.0033\n  -> mAP @ IoU=0.90 = 0.0005\n  -> mAP @ IoU=0.95 = 0.0000\n\n--- FASTER R-CNN mAP RESULTS (Manual) ---\nmAP (IoU=0.50): 0.0387\nmAP (IoU=0.75): 0.0126\n-------------------------------------------\nmAP@[.5:.05:.95]: 0.0167\n-------------------------------------------\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}